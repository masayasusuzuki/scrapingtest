import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse
import time
import random
import html
import re
import pandas as pd

# Set page title and layout
st.set_page_config(page_title="ã¨ã‚‰ã°ãƒ¼ã‚† æ±‚äººæƒ…å ±æ¤œç´¢", layout="wide")
st.title("ã¨ã‚‰ã°ãƒ¼ã‚† æ±‚äººæƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")

# Show debug checkbox
debug_mode = st.sidebar.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰")
show_html = st.sidebar.checkbox("HTMLè¡¨ç¤º") if debug_mode else False
direct_listing = st.sidebar.checkbox("ä¸€è¦§ãƒšãƒ¼ã‚¸URLã‚’ç›´æ¥ä½¿ç”¨") if debug_mode else False

# User input
search_keyword = st.text_input("è·ç¨®åã‚„æ–½è¨­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šçœ‹è­·å¸« æ¸‹è°·ãƒ¡ãƒ‡ã‚£ã‚«ãƒ«ã‚¯ãƒªãƒ‹ãƒƒã‚¯ï¼‰")

# å–å¾—ä»¶æ•°ã®è¨­å®š
max_jobs = st.sidebar.slider("å–å¾—ã™ã‚‹æ±‚äººæ•°", min_value=1, max_value=30, value=10)

# Common headers to mimic a browser
def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Referer': 'https://toranet.jp/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0',
    }

# Function to validate URL - ensure we only process relevant URLs
def is_valid_job_url(url):
    if not url:
        return False
    
    # Skip favorite_jobs and other irrelevant paths
    invalid_paths = ['favorite_jobs', 'login', 'register', 'contact', 'about']
    for path in invalid_paths:
        if path in url:
            return False
    
    # Only allow toranet.jp URLs or URLs that contain job-related terms
    valid = (
        ('toranet.jp' in url and ('job' in url or 'kyujin' in url or 'prefectures' in url)) or
        ('job_detail' in url)
    )
    
    if debug_mode and not valid:
        st.warning(f"ç„¡åŠ¹ãªURLã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {url}")
        
    return valid

# Function to extract phone number from text
def extract_phone_number(text):
    if not text:
        return None
    
    # é›»è©±ç•ªå·ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¸‚å¤–å±€ç•ª-å¸‚å†…å±€ç•ª-ç•ªå·ï¼‰
    patterns = [
        r'0\d{1,4}[-(]?\d{1,4}[)-]?\d{3,4}',  # 03-1234-5678 or 03(1234)5678
        r'é›»è©±ç•ªå·.{1,5}(0\d{1,4}[-(]?\d{1,4}[)-]?\d{3,4})',  # ã€Œé›»è©±ç•ªå·ï¼š03-1234-5678ã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        r'TEL.{1,5}(0\d{1,4}[-(]?\d{1,4}[)-]?\d{3,4})',  # ã€ŒTELï¼š03-1234-5678ã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        r'Tel.{1,5}(0\d{1,4}[-(]?\d{1,4}[)-]?\d{3,4})',  # ã€ŒTelï¼š03-1234-5678ã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        r'é›»è©±.{1,5}(0\d{1,4}[-(]?\d{1,4}[)-]?\d{3,4})'   # ã€Œé›»è©±ï¼š03-1234-5678ã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
    ]
    
    for pattern in patterns:
        matches = re.search(pattern, text, re.IGNORECASE)
        if matches:
            # ã‚°ãƒ«ãƒ¼ãƒ—ãŒã‚­ãƒ£ãƒ—ãƒãƒ£ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã€ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å…¨ä½“ã‚’è¿”ã™
            return matches.group(1) if len(matches.groups()) > 0 else matches.group(0)
    
    return None

# Function to make requests with retry logic
def make_request(url, max_retries=3):
    # Validate URL before sending request
    if not is_valid_job_url(url):
        return None, f"ç„¡åŠ¹ãªURL: {url}"
    
    for attempt in range(max_retries):
        try:
            # Add a random delay between requests
            if attempt > 0:
                time.sleep(random.uniform(2, 5))
            
            if debug_mode:
                st.info(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­: {url}")
            response = requests.get(url, headers=get_headers(), timeout=15)
            if debug_mode:
                st.success(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            response.raise_for_status()
            return response, None
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 503 and attempt < max_retries - 1:
                st.warning(f"ã‚µãƒ¼ãƒãƒ¼ãŒä¸€æ™‚çš„ã«åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å†è©¦è¡Œä¸­... ({attempt+1}/{max_retries})")
                continue
            return None, f"HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} - {e}"
        except requests.exceptions.RequestException as e:
            return None, f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    return None, "æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# Function to display HTML response
def display_html_response(response, title):
    if show_html and response:
        with st.expander(f"{title} - HTMLè¡¨ç¤º"):
            st.code(html.escape(response.text[:10000]) + "\n...(çœç•¥)..." if len(response.text) > 10000 else html.escape(response.text), language="html")

# Function to find all potential job detail links
def find_all_job_links(soup, search_url):
    all_links = []
    
    # Get all links from the page
    for a_tag in soup.find_all('a', href=True):
        href = a_tag.get('href')
        
        # Skip empty links
        if not href:
            continue
            
        # Ensure absolute URL
        if not href.startswith('http'):
            if href.startswith('/'):
                href = f"https://toranet.jp{href}"
            else:
                href = f"https://toranet.jp/{href}"
        
        # Only include valid job URLs
        if is_valid_job_url(href):
            text = a_tag.get_text().strip()
            all_links.append({
                'href': href,
                'text': text,
                'classes': a_tag.get('class', []),
                'id': a_tag.get('id', ''),
                'contains_detail_text': 'è©³ç´°' in text or 'detail' in href.lower() or 'æ±‚äºº' in text
            })
    
    if debug_mode:
        st.write(f"ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã—ãŸãƒªãƒ³ã‚¯æ•°: {len(all_links)}")
        
        # Show potential job links in debug mode
        with st.expander("æ½œåœ¨çš„ãªæ±‚äººãƒªãƒ³ã‚¯"):
            for i, link in enumerate(all_links[:20]):  # Show first 20 only
                st.write(f"{i+1}. [{link['text']}]({link['href']}) - ã‚¯ãƒ©ã‚¹: {link['classes']}")
    
    # First try links with detail-related text
    detail_links = [link for link in all_links if link['contains_detail_text']]
    
    # Next try to find links that look like job detail URLs
    pattern_links = [link for link in all_links if re.search(r'job.*detail|kyujin|recruit', link['href'])]
    
    # Combine and remove duplicates (keeping the original order)
    combined_links = []
    seen_urls = set()
    
    for link in detail_links + pattern_links:
        if link['href'] not in seen_urls:
            combined_links.append(link)
            seen_urls.add(link['href'])
    
    # As a last resort, add other valid links
    for link in all_links:
        if link['href'] not in seen_urls and search_url not in link['href'] and 'toranet.jp' in link['href']:
            combined_links.append(link)
            seen_urls.add(link['href'])
    
    # Extract just the URLs
    result_urls = [link['href'] for link in combined_links]
    
    if debug_mode:
        st.write(f"å–å¾—ã—ãŸæ±‚äººãƒªãƒ³ã‚¯æ•°: {len(result_urls)}")
    
    return result_urls[:max_jobs]  # Return only up to max_jobs links

# Function to scrape job listings
def get_job_listings(keyword):
    # Create search URL
    encoded_keyword = urllib.parse.quote(keyword)
    search_url = f"https://toranet.jp/prefectures/tokyo/job_search/kw/{encoded_keyword}"
    
    # If direct_listing is checked, use the search URL directly
    if direct_listing:
        st.info("ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ç›´æ¥è©³ç´°ãƒšãƒ¼ã‚¸ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™")
        return [search_url], None, search_url
    
    response, error = make_request(search_url)
    if error:
        return None, error, search_url
    
    # Display HTML for debugging
    display_html_response(response, "æ¤œç´¢çµæœãƒšãƒ¼ã‚¸")
    
    try:
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Advanced link finding approach - get multiple links
        job_links = find_all_job_links(soup, search_url)
        
        if not job_links:
            # If we couldn't find any suitable links, check if the search page itself has job details
            if any(tag.name in ['h1', 'h2'] and ('æ±‚äººæƒ…å ±' in tag.text or 'ä»•äº‹å†…å®¹' in tag.text) for tag in soup.find_all(['h1', 'h2'])):
                if debug_mode:
                    st.success("æ¤œç´¢ãƒšãƒ¼ã‚¸è‡ªä½“ãŒæ±‚äººè©³ç´°ãƒšãƒ¼ã‚¸ã®ã‚ˆã†ã§ã™ã€‚ç›´æ¥ä½¿ç”¨ã—ã¾ã™ã€‚")
                return [search_url], None, search_url
            
            return None, "æ±‚äººãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µã‚¤ãƒˆæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", search_url
        
        return job_links, None, search_url
    except Exception as e:
        st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        if debug_mode:
            st.code(traceback.format_exc(), language="python")
        return None, f"ãƒ‘ãƒ¼ã‚¹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", search_url

# Function to scrape job details
def get_job_details(detail_url):
    # Validate URL before processing
    if not is_valid_job_url(detail_url):
        return None, f"ç„¡åŠ¹ãªè©³ç´°ãƒšãƒ¼ã‚¸URL: {detail_url}"
    
    # Add a slight delay before making the next request
    time.sleep(random.uniform(0.5, 2))
    
    response, error = make_request(detail_url)
    if error:
        return None, error
    
    # Display HTML for debugging
    display_html_response(response, "è©³ç´°ãƒšãƒ¼ã‚¸")
    
    try:
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Debug - output all div classes to help identify correct selectors
        if debug_mode and show_html:
            with st.expander("ãƒšãƒ¼ã‚¸å†…ã®divè¦ç´ ã®ã‚¯ãƒ©ã‚¹ä¸€è¦§"):
                divs = soup.find_all('div', class_=True)
                for i, div in enumerate(divs[:30]):  # Limit to first 30 to avoid clutter
                    st.write(f"{i+1}. Class: {div.get('class')} - ãƒ†ã‚­ã‚¹ãƒˆ: {div.text[:50]}")
            
            # Also show all headings
            with st.expander("ãƒšãƒ¼ã‚¸å†…ã®è¦‹å‡ºã—è¦ç´ "):
                headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                for i, h in enumerate(headings):
                    st.write(f"{i+1}. {h.name}: {h.text.strip()}")
        
        # Extract facility name - try multiple selectors
        facility_name = "æƒ…å ±ãªã—"
        facility_name_selectors = [
            'div.corpNameWrap > span', 
            'div.corpName', 
            'h1.company-name',
            'div.company-name',
            'h1', # Try any h1 tag
            'h2', # Try any h2 tag
            'div.corpInfo', # Try corporation info div
            'span.name', # Try name span
            '.corp-name', # Try corp-name class
            '.company' # Try company class
        ]
        
        for selector in facility_name_selectors:
            facility_name_element = soup.select_one(selector)
            if facility_name_element:
                facility_name = facility_name_element.text.strip()
                if debug_mode and show_html:
                    st.success(f"æ–½è¨­åãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆã‚»ãƒ¬ã‚¯ã‚¿: {selector}ï¼‰")
                break
        
        # Fallback: Try to find text that looks like a company name
        if facility_name == "æƒ…å ±ãªã—":
            # Look for text that might be a company name (often near the top of the page)
            top_elements = soup.find_all(['div', 'span', 'p'], limit=20)
            for elem in top_elements:
                text = elem.text.strip()
                # Company names typically aren't very long and don't contain certain patterns
                if 5 < len(text) < 50 and ('æ ªå¼ä¼šç¤¾' in text or 'æœ‰é™ä¼šç¤¾' in text or 'ç—…é™¢' in text or 'ã‚¯ãƒªãƒ‹ãƒƒã‚¯' in text):
                    facility_name = text
                    if debug_mode and show_html:
                        st.success(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ–½è¨­åã‚’æ¤œå‡º: {text}")
                    break
        
        # Extract phone number
        phone_number = "æƒ…å ±ãªã—"
        
        # Try to find phone number in specific elements first
        phone_selectors = [
            'div.tel', 
            'div.phone',
            'span.tel',
            'span.phone',
            'p.tel',
            'a[href^="tel:"]',
            'div.contact'
        ]
        
        for selector in phone_selectors:
            phone_elements = soup.select(selector)
            for element in phone_elements:
                element_text = element.text.strip()
                extracted_number = extract_phone_number(element_text)
                if extracted_number:
                    phone_number = extracted_number
                    if debug_mode and show_html:
                        st.success(f"é›»è©±ç•ªå·ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆã‚»ãƒ¬ã‚¯ã‚¿: {selector}ï¼‰: {phone_number}")
                    break
            if phone_number != "æƒ…å ±ãªã—":
                break
        
        # If not found, search in the entire page text
        if phone_number == "æƒ…å ±ãªã—":
            # Look for phone number in the entire page
            page_text = soup.get_text()
            extracted_number = extract_phone_number(page_text)
            if extracted_number:
                phone_number = extracted_number
                if debug_mode and show_html:
                    st.success(f"ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰é›»è©±ç•ªå·ã‚’æ¤œå‡º: {phone_number}")
        
        # Extract website URL - try multiple selectors
        website_url = "æƒ…å ±ãªã—"
        website_selectors = [
            'div.corpLink a', 
            'a.company-url', 
            'a[href*="http"]',
            'a[target="_blank"]' # Links that open in new window are often external website links
        ]
        
        for selector in website_selectors:
            elements = soup.select(selector)
            for element in elements:
                href = element.get('href', '')
                # Skip links to toranet.jp and very short URLs
                if href and len(href) > 10 and 'toranet.jp' not in href and href.startswith('http'):
                    website_url = href
                    if debug_mode and show_html:
                        st.success(f"Webã‚µã‚¤ãƒˆURLãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆã‚»ãƒ¬ã‚¯ã‚¿: {selector}ï¼‰")
                    break
            if website_url != "æƒ…å ±ãªã—":
                break
        
        # Extract job description - try multiple selectors
        job_description = "æƒ…å ±ãªã—"
        description_selectors = [
            'div.jobDtlText.jobIntro', 
            'div.job-description', 
            'div.description',
            'div[class*="job"][class*="description"]',
            'div[class*="description"]',
            'div.jobDetail',
            'div.jobContent',
            'div.kyujin-detail',
            'section.detail',
            'div.detail-content',
            'div.job-content'
        ]
        
        for selector in description_selectors:
            job_description_elements = soup.select(selector)
            if job_description_elements:
                # Combine all matching elements
                combined_text = "\n\n".join([elem.text.strip() for elem in job_description_elements])
                if combined_text:
                    job_description = combined_text
                    if debug_mode and show_html:
                        st.success(f"æ¥­å‹™å†…å®¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆã‚»ãƒ¬ã‚¯ã‚¿: {selector}ï¼‰")
                    break
        
        # Enhanced fallback mechanism for job description
        if job_description == "æƒ…å ±ãªã—":
            # Try to find sections with job-related keywords
            keywords = ['ä»•äº‹å†…å®¹', 'æ¥­å‹™å†…å®¹', 'è·å‹™å†…å®¹', 'ãŠä»•äº‹', 'è·ç¨®']
            
            # First look for headings with these keywords
            for keyword in keywords:
                elements = soup.find_all(string=re.compile(keyword))
                if elements:
                    # For each element containing the keyword, look for nearby content
                    for element in elements:
                        parent = element.parent
                        # Try to get content from the next sibling or parent's next sibling
                        content = None
                        if parent.next_sibling:
                            content = parent.next_sibling
                        elif parent.parent and parent.parent.next_sibling:
                            content = parent.parent.next_sibling
                        
                        if content and hasattr(content, 'text'):
                            job_description = content.text.strip()
                            if debug_mode and show_html:
                                st.success(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã‹ã‚‰æ¥­å‹™å†…å®¹ã‚’æ¤œå‡º")
                            break
                
                if job_description != "æƒ…å ±ãªã—":
                    break
            
            # If still not found, try to look for any substantial text blocks
            if job_description == "æƒ…å ±ãªã—":
                main_content_divs = soup.select('div.main-content, div.content, div.detail, div.job, article, section')
                for div in main_content_divs:
                    paragraphs = div.find_all(['p', 'div'], class_=lambda c: c and ('text' in c or 'content' in c))
                    if paragraphs:
                        job_description = "\n\n".join([p.text.strip() for p in paragraphs])
                        if debug_mode and show_html:
                            st.success("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹æ³•ã§æ¥­å‹™å†…å®¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                        break
        
        # Get a shorter version of the job description for the table
        short_description = job_description[:100] + "..." if len(job_description) > 100 else job_description
        
        # Debug information - only show for HTML debug mode
        if debug_mode and show_html:
            st.info("### ãƒ‡ãƒãƒƒã‚°æƒ…å ± (é–‹ç™ºè€…å‘ã‘) ###")
            st.info(f"æ–½è¨­åã‚»ãƒ¬ã‚¯ã‚¿ã®çµæœï¼š{facility_name}")
            st.info(f"é›»è©±ç•ªå·ï¼š{phone_number}")
            st.info(f"Webã‚µã‚¤ãƒˆURLã‚»ãƒ¬ã‚¯ã‚¿ã®çµæœï¼š{website_url}")
            st.info(f"æ¥­å‹™å†…å®¹ã‚»ãƒ¬ã‚¯ã‚¿ã®çµæœï¼š{short_description}")
        
        return {
            "facility_name": facility_name,
            "phone_number": phone_number,
            "website_url": website_url,
            "job_description": job_description,
            "short_description": short_description,
            "source_url": detail_url
        }, None
    except Exception as e:
        st.error(f"è©³ç´°æƒ…å ±ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        if debug_mode:
            import traceback
            st.code(traceback.format_exc(), language="python")
        return None, f"è©³ç´°æƒ…å ±ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# Function to display job details in a table
def display_job_table(job_list):
    # Prepare data for the table
    table_data = []
    for job in job_list:
        table_data.append({
            "æ–½è¨­å": job['facility_name'],
            "é›»è©±ç•ªå·": job['phone_number'],
            "æ¥­å‹™å†…å®¹": job['short_description'],
            "è©³ç´°": f"[è©³ç´°ã‚’è¡¨ç¤º]({job['source_url']})",
            "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ": f"[ã‚µã‚¤ãƒˆã¸]({job['website_url']})" if job['website_url'] != "æƒ…å ±ãªã—" else "æƒ…å ±ãªã—"
        })
    
    # Convert to DataFrame and display
    df = pd.DataFrame(table_data)
    st.dataframe(df, use_container_width=True)

# Function to display full job details
def display_full_job_details(job):
    with st.expander(f"ã€è©³ç´°ã€‘{job['facility_name']}"):
        st.markdown(f"**æ–½è¨­å**: {job['facility_name']}")
        st.markdown(f"**é›»è©±ç•ªå·**: {job['phone_number']}")
        
        if job['website_url'] != "æƒ…å ±ãªã—":
            st.markdown(f"**ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ**: [{job['website_url']}]({job['website_url']})")
        else:
            st.markdown("**ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ**: æƒ…å ±ãªã—")
        
        st.markdown("**æ¥­å‹™å†…å®¹**:")
        st.markdown(job['job_description'])
        
        st.markdown(f"[å…ƒã®æ±‚äººæƒ…å ±ã‚’è¦‹ã‚‹]({job['source_url']})")

# Test direct URL access
direct_url = st.sidebar.text_input("ç›´æ¥URLã‚’å…¥åŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰") if debug_mode else None

# Search logic
if direct_url and debug_mode:
    st.info(f"ç›´æ¥å…¥åŠ›ã•ã‚ŒãŸURLã‚’ä½¿ç”¨: {direct_url}")
    with st.spinner('URLã‹ã‚‰æƒ…å ±ã‚’å–å¾—ä¸­...'):
        job_details, error = get_job_details(direct_url)
        
        if error:
            st.error(error)
        elif job_details:
            # Display job details
            display_full_job_details(job_details)
elif search_keyword:
    with st.spinner('æ¤œç´¢ä¸­...'):
        job_links, error, search_url = get_job_listings(search_keyword)
        
        # Display search URL for debugging
        if debug_mode:
            st.markdown(f"æ¤œç´¢URL: [{search_url}]({search_url})")
        
        if error:
            st.error(error)
            if debug_mode:
                st.error("ã‚»ãƒ¬ã‚¯ã‚¿ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
                st.markdown(f"[æ¤œç´¢çµæœã‚’ç›´æ¥ç¢ºèªã™ã‚‹]({search_url})")
        elif job_links:
            job_list = []
            total_jobs = len(job_links)
            
            # Create a progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, link in enumerate(job_links):
                status_text.text(f"æ±‚äººæƒ…å ±ã‚’å–å¾—ä¸­... ({idx+1}/{total_jobs})")
                progress_bar.progress((idx+1)/total_jobs)
                
                if debug_mode:
                    st.info(f"æ±‚äººè©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™: {link}")
                
                job_details, error = get_job_details(link)
                
                if error:
                    if debug_mode:
                        st.error(f"è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {error}")
                        st.markdown(f"[è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ç›´æ¥ç¢ºèªã™ã‚‹]({link})")
                elif job_details:
                    job_list.append(job_details)
            
            # Clear progress indicators
            progress_bar.empty()
            status_text.empty()
            
            # Display results
            if job_list:
                st.success(f"{len(job_list)}ä»¶ã®æ±‚äººæƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
                
                # Display jobs in a table
                display_job_table(job_list)
                
                # Show full details in expandable sections
                st.subheader("ğŸ“‹ è©³ç´°æƒ…å ±")
                for job in job_list:
                    display_full_job_details(job)
            else:
                st.warning("æ±‚äººæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    st.info("ä¸Šã®æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã«è·ç¨®åã‚„æ–½è¨­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    if debug_mode:
        st.info("ã¾ãŸã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ç›´æ¥URLã‚’å…¥åŠ›ã—ã¦ãƒ‡ãƒãƒƒã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚") 